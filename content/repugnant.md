+++
title = "Repugnant astronomical ruin"
date = 2022-06-05
description = "A note on utilitarianism and the long term."
+++

Context: I'm not a utilitarian. 

This isn't why I'm not a utilitarian—it's just me playing with some perennial debates reappearing alongside [Michael Nielsen's notes](https://michaelnotebook.com/eanotes/), [requests for critical writing](https://forum.effectivealtruism.org/posts/8hvmvrgcxJJ2pYR4X/announcing-a-contest-ea-criticism-and-red-teaming) on Effective Altruism, and [prompts](https://www.openphilanthropy.org/blog/cause-exploration-prizes) from Open Philanthropy. I do think utilitarians (and I'm in particular thinking of longtermist Effective Altruists) should take standard objections more seriously and be more self-skeptical in their reasoning. (I'm also not an Effective Altruist, although I support GiveWell.) This has all probably been said before.

### The repugnant conclusion

The [mere addition paradox](https://en.wikipedia.org/wiki/Mere_addition_paradox) is a bullet commonly bitten in utilitarian ethics. It leads to the so-called "repugnant conclusion" that a large number of lives "barely worth living" (that is, with welfare just sufficient to yield a positive contribution to the aggregate utility function) can be better than a smaller number of higher-welfare lives—that, in fact, in any given positive-utility scenario, there is always a preferable larger, lower-welfare population. This kind of utilitarianism is assumed in the discussions below.

### Astronomical waste

"Astronomical waste" is a term from a short article by [Nick Bostrom](https://nickbostrom.com/astronomical/waste) pointing out that not only can a spacefaring future be radically large—large enough for existential risk{% note() %}that is, risk of events that preclude such a future {% end %} to dominate expected utility calculations—but also any delay in intergalactic expansion sacrifices astronomical potential utility.

[Christian Tarsney observes](https://globalprioritiesinstitute.org/wp-content/uploads/Tarsney-Epistemic-Challenge-to-Longtermism.pdf) that it might be important, against mere cubic interstellar expansion, to include a regularizing exponential decay in expected utility with time owing to a constant risk of extinction (or other event nullifying a decision's impact on the longterm future).{% note() %} I've emailed the author about this, but note that the Virgo supercluster density used is too large by a factor of ten (it should be 2.9 × 10^{-10} if it hasn't already been corrected), affecting the very long-term scenarios accordingly.{% end %} Under uncertainty, even given a small chance that nullifying events are rare enough, the next million to billion years allow existential risk reduction to outweigh near-term improvements to welfare.

### Gambler's ruin

A persistent gambler playing double or nothing [will eventually go broke](https://en.wikipedia.org/wiki/Gambler%27s_ruin), even if each round has positive expected value.

### Repugnant astronomical ruin

A life barely worth living is still worth living, of course, so one might object to name-calling. But the repugnance, I think, arises from the existence of a "worth living" threshold.

If the threshold is low—say, all lives today are worth living, or even all non-suicidal lives—then a superior future is full of people we would today judge to be suffering terribly. If you bite that bullet, I'm not sure how to talk to you. If the threshold is higher, then many consensually alive today don't pass.{% note() %} Peli Grietzer [has been tweeting](https://twitter.com/peligrietzer/status/1533384731701792770) about this. {% end %}

While this is contrary to intuition, one can still bite the high-threshold bullet. But even if it's so, the value of the longterm future now depends on establishing, as a civilization, that just "OK" (or better, as it may be) isn't good enough. And at that point it sounds quite easy to accidentally create an astronomically bad future.

A calculation like Tarsney's above relies on a distribution of expected utility that favors positive valence. In this case there's good reason to expect otherwise: modern humans, at least, apparently reliably set their threshold too low.{% note() %} There are good-faith arguments that [this really is true](https://en.wikipedia.org/wiki/Antinatalism#Realism) in some sense. {% end %} Moreover, a high bar makes [asymmetry](https://reducing-suffering.org/happiness-suffering-symmetric/) more plausible. As with other [s-risk](https://en.wikipedia.org/wiki/Suffering_risks) scenarios, you could conceivably reorient your longtermist efforts towards averting it. But even more so than with other longterm considerations, there's a plausible argument that improving welfare today is the best way to do so.

Actually, it's worse than that. Say our future civilization's advanced cyberneurophilosophy gets it right, and they can accurately calculate expected utility of their own decisions. They continue to recognize that the potential utility of rapid interstellar (and, later, intergalactic) colonization dominates.

But if contemporary welfare trades off against productivity and further existential risk reduction, they'll keep putting off prioritizing welfare indefinitely—until they meet ruin.{% note() %} Or, perhaps, now your weight on the longterm future reduces to your confidence that they correctly, collectively implement something like the [Kelly criterion](https://en.wikipedia.org/wiki/Kelly_criterion) and balance how much present welfare they stake and how much they bank. But remember, we're betting in pure (not log) utility, which Kelly doesn't maximize. {% end %}

A more sophisticated argument is that only once the "eschatological bound" is close enough will a surviving civilization enter the "value realization" stage, but that's much further in the future than the epoch available to our present exponentially decaying expected utility envelope.